# -*- coding: utf-8 -*-
"""prediction_churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rB6W-0529MXzexBCL_cvgq7jZpOnAvMt
"""

# pip install imblearn
# BIBLIOTECAS -------------------------------------------------------------------------------------------------------------
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold as skf
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, classification_report,
                             roc_curve, ConfusionMatrixDisplay)

from sklearn.compose import ColumnTransformer                   # For applying different preprocessing to columns
from sklearn.ensemble import RandomForestClassifier
9
from imblearn.combine import SMOTEENN
from imblearn.pipeline import Pipeline as ImbPipeline

from yellowbrick.classifier import ClassificationReport
# import torch
# device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"
# print(f"Using {device} device")

"""## EDA - Analise Exploratória de dados """ #--------------------------------------------------------------------------------------------------

df = pd.read_csv(r'C:\Users\gabri\Documents\Machine Learning\DataSets\WA_Fn-UseC_-Telco-Customer-Churn.csv') # 1 = cancelou; 0 = permaneceu


df['MultipleLines'].unique()
df['InternetService'].unique()

df.info()

#não tem nulos , não tem duplicatas

df['Churn'].value_counts()/len(df)

df2 = df.copy()
df2['Contract'].unique() # One hot encoding
df2['PaymentMethod'].unique() # One hot encoding

df.describe(include=['O'])

df['TotalCharges']

"""## Pre processamento""" #--------------------------------------------------------------------------------------------------

df2['MonthlyCharges'] = pd.to_numeric(df2['MonthlyCharges'], errors= 'coerce')
df2['TotalCharges'] = pd.to_numeric(df2['TotalCharges'], errors= 'coerce')

df2.isna().sum()

df2 = df2.dropna(subset =['TotalCharges'])
df2['TotalCharges'].isna().sum()

# def scaler(df, col):
#   stardard_scaler = StandardScaler()
#   df[col] = stardard_scaler.fit_transform(df[[col]])
#   return df

# df2['MonthlyCharges'] = scaler(df2, 'MonthlyCharges')
# df2['TotalCharges'] = scaler(df2, 'TotalCharges')

X = df2.drop(['customerID','Churn'],axis = 1)
y = df2['Churn'].map({'Yes': 1, 'No': 0})
# y = df2['Churn'].apply(lambda x : 1 if x == 'Yes' else 0)

df2['Churn'].value_counts()

if y.isnull().any():
    print("Há valores não mapeados ou nulos em df['Churn'].")

"""### Build preprocessing pipelines""" #--------------------------------------------------------------------------------------------------

categorical_columns = X.select_dtypes(include =['object']).columns.tolist() #→ Filtra e retorna apenas as colunas que possuem o tipo object, isto é, colunas que armazenam dados não numéricos, como strings ou textos.
numerical_columns = X.select_dtypes(include =['int64', 'float64']).columns.tolist()

numeric_transformer = Pipeline(steps= [
    ('scaler',StandardScaler())
])

categorical_transformer = Pipeline(steps= [
    ('onehot',OneHotEncoder(drop='first',
                            sparse_output=False,
                            handle_unknown ='ignore'))
])

preprocessor =  ColumnTransformer(transformers = [
    ('num', numeric_transformer, numerical_columns),
    ('cat', categorical_transformer, categorical_columns)
])

#4.5 Handle class imbalance using SMOTE + Edited Nearest Neighbors

smote_enn = SMOTEENN(random_state=42)

#full pipeline:
pipeline = ImbPipeline(steps = [
    ('preprocessor', preprocessor),
    ('smote_enn', smote_enn)
])

#apply transformations
X_resampled, y_resampled = pipeline.fit_resample(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_resampled,
                                                    y_resampled,
                                                    test_size=0.2,
                                                    random_state=42,
                                                    stratify = y_resampled)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs = -1)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:,1] # probability of positive class (Churn)


# Print key metrics
print("Default Random Forest Performance")
print(f"F1-score : {f1_score(y_test, y_pred):.4f}")
print(f"AUC-ROC  : {roc_auc_score(y_test, y_proba):.4f}\n")
print(classification_report(y_test, y_pred))

"""#Hyperparameter Tuning with""" #--------------------------------------------------------------------------------------------------

# !pip install optuna

import optuna
from sklearn.model_selection import cross_val_score

def objective(trial,X, y):
  params = {
      'max_depth': trial.suggest_int('max_depth',1,50),
      'n_estimators': trial.suggest_int('n_estimators',100,1000),
      'min_samples_split': trial.suggest_int('min_samples_split',2,10),
      'min_samples_leaf': trial.suggest_int('min_samples_leaf',1,10),
  }

  cv = skf(n_splits=5, shuffle=True, random_state=42)
  metric_list = []

  # Uso de .iloc em X_resampled e y_resampled pode falhar se forem NumPy arrays (e não DataFrames).

  for c, (train_index, test_index) in enumerate(cv.split(X, y)):
      x_train, x_test = X[train_index], X[test_index]
      y_train, y_test = y[train_index], y[test_index]

      rf = RandomForestClassifier(**params)
      rf.fit(x_train, y_train)
      y_pred = rf.predict(x_test)
      metric_list.append(f1_score(y_test, y_pred))

  return np.mean(metric_list)

def optimize_hyperparameters(X, y):
  study = optuna.create_study(direction='maximize')

  study.optimize(lambda trial: objective(trial, X, y), n_trials=25)

  return study.best_params

best_params = optimize_hyperparameters(X_resampled, y_resampled)
print(best_params)

import pickle
with open('best_params.pkl', 'wb') as f:
    pickle.dump(best_params, f)

def pred_data(X,y):
    cv = skf(n_splits=5, shuffle=True, random_state=42)
    metric_list = []

      # Uso de .iloc em X_resampled e y_resampled pode falhar se forem NumPy arrays (e não DataFrames).
    for c, (train_index, test_index) in enumerate(cv.split(X, y)):
        x_train, x_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        rf = RandomForestClassifier(**best_params)
        rf.fit(x_train, y_train)
        y_pred = rf.predict(x_test)
        y_proba = rf.predict_proba(x_test)[:,1]
        metric_list.append(f1_score(y_test, y_pred))

    return np.mean(metric_list)

resultf1 = pred_data(X_resampled, y_resampled)
print(resultf1)

"""## Yellowbricks"""

# !pip install Yellowbrick

from yellowbrick.classifier import ClassificationReport

def pred_data(X,y):
    cv = skf(n_splits=5, shuffle=True, random_state=42)
    metric_list = []

      # Uso de .iloc em X_resampled e y_resampled pode falhar se forem NumPy arrays (e não DataFrames).
    for c, (train_index, test_index) in enumerate(cv.split(X, y)):
        x_train, x_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        rf = RandomForestClassifier(**best_params)
        rf.fit(x_train, y_train)
        y_pred = rf.predict(x_test)
        y_proba = rf.predict_proba(x_test)[:,1]
        metric_list.append(f1_score(y_test, y_pred))

        visualizer =  ClassificationReport(rf, classes = ['No', 'Yes'], support = True)
        visualizer.score(x_test, y_test)
        visualizer.show()

    return np.mean(metric_list)

resultf1 = pred_data(X_resampled, y_resampled)
print(resultf1)

